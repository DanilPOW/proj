import fitz
from docx import Document
from docx.shared import RGBColor
from docx.enum.text import WD_COLOR_INDEX
import tempfile
import os
import zipfile
import xml.etree.ElementTree as ET
import re
from lxml import etree
from fuzzywuzzy import fuzz

def replace_cyrillic_with_latin_extended(text):
    """
    –†–ê–°–®–ò–†–ï–ù–ù–ê–Ø –∑–∞–º–µ–Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –ª–∞—Ç–∏–Ω—Å–∫–∏–µ
    """
    cyrillic_to_latin = {
        # –û—Å–Ω–æ–≤–Ω—ã–µ –±—É–∫–≤—ã
        '–ê': 'A', '–∞': 'a',
        '–í': 'V', '–≤': 'v', 
        '–ï': 'E', '–µ': 'e',
        '–ö': 'K', '–∫': 'k',
        '–ú': 'M', '–º': 'm',
        '–ù': 'H', '–Ω': 'h',
        '–û': 'O', '–æ': 'o',
        '–†': 'P', '—Ä': 'p',
        '–°': 'C', '—Å': 'c',
        '–¢': 'T', '—Ç': 't',
        '–£': 'U', '—É': 'u',
        '–•': 'X', '—Ö': 'x',
        '–Ö': 'S', '—ï': 's',
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
        'ùê∏': 'E', 'ùëö': 'm', 'ùëê': 'c',
        'ùëà': 'U', 'ùëÖ': 'R', 'ùêº': 'I',
        
        # –ï–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
        '–∫–û–º': 'kOm', '–û–º': 'Om',
        '–∫–≥': 'kg', '–º/—Å': 'm/s',
        '–ì—Ü': 'Hz', '–í—Ç': 'W',
    }
    
    result = text
    for cyr, lat in cyrillic_to_latin.items():
        result = result.replace(cyr, lat)
    
    return result

def separate_merged_variables(text):
    """
    –†–∞–∑–¥–µ–ª—è–µ—Ç —Å–ª–∏—Ç—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–∏–ø–∞ VR -> V/R, UI -> U/I
    """
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è —Å–ª–∏—Ç—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    patterns = [
        # –î–≤–µ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã –ø–æ–¥—Ä—è–¥ –≤ –∫–æ–Ω—Ü–µ –∏–ª–∏ –ø–µ—Ä–µ–¥ =
        (r'\b([A-Z])([A-Z])\b(?=\s*=|\s*$)', r'\1/\2'),
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–ª—É—á–∞–∏ –¥–ª—è —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º—É–ª
        (r'\bUI\b', 'U/I'),
        (r'\bVR\b', 'V/R'), 
        (r'\bIR\b', 'I/R'),
        (r'\bPV\b', 'P/V'),
        (r'\bFm\b', 'F/m'),
        
        # –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è + —Ü–∏—Ñ—Ä–∞ (—Å—Ç–µ–ø–µ–Ω—å)
        (r'\b([A-Za-z])\s*(\d)\b', r'\1^\2'),
    ]
    
    result = text
    for pattern, replacement in patterns:
        old_result = result
        result = re.sub(pattern, replacement, result)
        if result != old_result:
            print(f"    üîß –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ü–ï–†–ï–ú–ï–ù–ù–´–•: '{old_result}' -> '{result}'")
    
    return result

def fix_pdf_formula_structure(text):
    """
    –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï —Å—Ç—Ä—É–∫—Ç—É—Ä—ã PDF —Ñ–æ—Ä–º—É–ª —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    """
    if not text:
        return text
    
    original_text = text
    
    # 1. –ó–∞–º–µ–Ω—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
    text = replace_cyrillic_with_latin_extended(text)
    
    # 2. –ò—Å–ø—Ä–∞–≤–ª—è–µ–º Unicode —Å–∏–º–≤–æ–ª—ã
    unicode_fixes = {
        '‚àô': '*', '√ó': '*', '‚ãÖ': '*', '¬∑': '*',
        '‚àí': '-', '‚Äì': '-', '‚Äî': '-',
        '√∑': '/', '¬≤': '^2', '¬≥': '^3',
        '===': '=', '==': '=',
    }
    
    for old, new in unicode_fixes.items():
        text = text.replace(old, new)
    
    # 3. –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê: —Ü–∏—Ñ—Ä–∞ –≤ –Ω–∞—á–∞–ª–µ + –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    # "2ùê∏= ùëöùëê" -> "E=mc^2"
    match = re.match(r'^(\d+)\s*([A-Za-z])\s*=\s*(.+)$', text)
    if match:
        digit, variable, rest = match.groups()
        if digit in ['2', '3', '4', '5']:
            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Å—Ç–µ–ø–µ–Ω–∏
            rest_parts = rest.split()
            if rest_parts:
                last_part = rest_parts[-1]
                if len(last_part) == 1 and last_part.isalpha():
                    rest_parts[-1] = f"{last_part}^{digit}"
                    rest = ' '.join(rest_parts)
                elif 'c' in rest.lower():
                    rest = re.sub(r'\bc\b', f'c^{digit}', rest, flags=re.IGNORECASE)
            
            text = f"{variable}={rest}"
            print(f"    üîß –ò–°–ü–†–ê–í–õ–ï–ù –ü–û–†–Ø–î–û–ö –°–¢–ï–ü–ï–ù–ò: '{original_text}' -> '{text}'")
    
    # 4. –†–∞–∑–¥–µ–ª—è–µ–º —Å–ª–∏—Ç—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    text = separate_merged_variables(text)
    
    # 5. –£–ú–ù–ê–Ø –æ–±—Ä–µ–∑–∫–∞ –ø–æ –µ–¥–∏–Ω–∏—Ü–∞–º –∏–∑–º–µ—Ä–µ–Ω–∏—è
    # –ò—â–µ–º –ø–µ—Ä–≤–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ –µ–¥–∏–Ω–∏—Ü, –Ω–æ —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –∑–Ω–∞–∫–∞ =
    if '=' in text:
        eq_pos = text.find('=')
        after_eq = text[eq_pos:]
        
        units_pattern = r'\b(kOm|Om|Œ©|V|A|kg|m/s|Hz|W|–∫–û–º|–û–º|–í|–ê|–∫–≥|–º/—Å|–ì—Ü|–í—Ç)\b'
        unit_match = re.search(units_pattern, after_eq, flags=re.IGNORECASE)
        
        if unit_match:
            # –û–±—Ä–µ–∑–∞–µ–º –¥–æ –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
            cut_pos = eq_pos + unit_match.start()
            before_unit = text[:cut_pos].strip()
            
            # –£–±–∏—Ä–∞–µ–º –∑–∞–ø—è—Ç—ã–µ –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –≤ –∫–æ–Ω—Ü–µ
            before_unit = re.sub(r'[,;.\s]+$', '', before_unit)
            
            if len(before_unit) >= 3 and '=' in before_unit:
                text = before_unit
                print(f"    ‚úÇÔ∏è –û–ë–†–ï–ó–ö–ê –ü–û –ï–î–ò–ù–ò–¶–ê–ú: '{original_text}' -> '{text}'")
    
    # 6. –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    text = re.sub(r'\s+', ' ', text.strip())
    text = re.sub(r'\s*([=+\-*/^])\s*', r'\1', text)  # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –≤–æ–∫—Ä—É–≥ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤
    
    if text != original_text:
        print(f"    üßπ PDF –°–¢–†–£–ö–¢–£–†–ù–ê–Ø –û–ß–ò–°–¢–ö–ê: '{original_text}' -> '{text}'")
    
    return text

def fix_docx_formula_structure(text):
    """
    –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï —Å—Ç—Ä—É–∫—Ç—É—Ä—ã DOCX —Ñ–æ—Ä–º—É–ª
    """
    if not text:
        return text
    
    original_text = text
    
    # 1. –ó–∞–º–µ–Ω—è–µ–º –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
    text = replace_cyrillic_with_latin_extended(text)
    
    # 2. –£–±–∏—Ä–∞–µ–º OMML –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    # "E=mc2c^2" -> "E=mc^2"
    text = re.sub(r'([a-z])(\d)([a-z])\^(\d)', r'\1\3^\2', text)  # mc2c^2 -> mc^2
    text = re.sub(r'([a-z])\^?(\d)\1\^(\d)', r'\1^\2', text)      # c2c^2 -> c^2
    
    # 3. –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –¥—Ä–æ–±–∏ –∏–∑ —Å–ª–∏—Ç—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    # "UI" -> "U/I", –Ω–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ —á–∞—Å—Ç—å –±–æ–ª—å—à–µ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
    restoration_patterns = [
        (r'\bUI\b(?!=)', 'U/I'),  # UI –Ω–µ –ø–µ—Ä–µ–¥ =
        (r'\bVR\b(?!=)', 'V/R'),
        (r'\bIR\b(?!=)', 'I/R'),
        (r'\bPV\b(?!=)', 'P/V'),
    ]
    
    for pattern, replacement in restoration_patterns:
        old_text = text
        text = re.sub(pattern, replacement, text)
        if text != old_text:
            print(f"    üîß –í–û–°–°–¢–ê–ù–û–í–õ–ï–ù–ê –î–†–û–ë–¨ DOCX: '{old_text}' -> '{text}'")
    
    # 4. –û–±—Ä–µ–∑–∞–µ–º –ø–æ—è—Å–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ö–≤–æ—Å—Ç—ã –†–ê–ù–¨–®–ï
    split_patterns = [
        r'[;,]\s*(?=[–ê-–ØA-Z])',  # ; –≥–¥–µ, , –ü—Ä–∏
        r'\s+–≥–¥–µ\s+',
        r'\s+–ø—Ä–∏\s+',
        r'\s+–¥–ª—è\s+',
        r'$$[^)]*$$$$[^)]*$$',   # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ —Å–∫–æ–±–∫–∏ (U)/(I)(2 –í)/(1‚àô10-3 –ê)
    ]
    
    for pattern in split_patterns:
        parts = re.split(pattern, text, maxsplit=1)
        if len(parts) > 1 and '=' in parts[0]:
            text = parts[0].strip()
            break
    
    # 5. –û–±—Ä–µ–∑–∞–µ–º –ø–æ –µ–¥–∏–Ω–∏—Ü–∞–º –∏–∑–º–µ—Ä–µ–Ω–∏—è
    units_pattern = r'\b(kOm|Om|Œ©|V|A|kg|m/s|Hz|W)\b'
    unit_matches = list(re.finditer(units_pattern, text, flags=re.IGNORECASE))
    
    if unit_matches:
        first_unit = unit_matches[0]
        before_unit = text[:first_unit.start()].strip()
        
        if '=' in before_unit and len(before_unit) >= 3:
            text = before_unit
            print(f"    ‚úÇÔ∏è DOCX –û–ë–†–ï–ó–ö–ê –ü–û –ï–î–ò–ù–ò–¶–ê–ú: '{original_text}' -> '{text}'")
    
    # 6. –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    text = re.sub(r'[,;.\s]+$', '', text)
    text = re.sub(r'=+', '=', text)
    text = re.sub(r'\s+', ' ', text.strip())
    text = re.sub(r'\s*([=+\-*/^])\s*', r'\1', text)
    
    if text != original_text:
        print(f"    üßπ DOCX –°–¢–†–£–ö–¢–£–†–ù–ê–Ø –û–ß–ò–°–¢–ö–ê: '{original_text}' -> '{text}'")
    
    return text

def tokenize_formula(text):
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ñ–æ—Ä–º—É–ª—É –Ω–∞ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    """
    if not text:
        return []
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–∫–µ–Ω—ã: –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, —á–∏—Å–ª–∞, –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
    tokens = re.findall(r'[A-Za-z]+|\d+(?:\^\d+)?|\d*\.\d+|[=+\-*/^<>‚â§‚â•‚â†‚âà]', text)
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–∫–µ–Ω—ã
    normalized_tokens = []
    for token in tokens:
        token = token.lower()
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
        if token in ['√ó', '‚àô', '¬∑', '‚ãÖ']:
            token = '*'
        elif token in ['‚àí', '‚Äì', '‚Äî']:
            token = '-'
        elif token in ['√∑']:
            token = '/'
        
        normalized_tokens.append(token)
    
    return normalized_tokens

def calculate_token_similarity(tokens1, tokens2):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–∫–µ–Ω–æ–≤ (Jaccard distance)
    """
    if not tokens1 or not tokens2:
        return 0.0
    
    set1 = set(tokens1)
    set2 = set(tokens2)
    
    intersection = set1 & set2
    union = set1 | set2
    
    if not union:
        return 0.0
    
    jaccard = len(intersection) / len(union)
    
    # –ë–æ–Ω—É—Å –∑–∞ –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
    length_bonus = 1 - abs(len(tokens1) - len(tokens2)) / max(len(tokens1), len(tokens2))
    
    # –ë–æ–Ω—É—Å –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤
    order_bonus = 0
    if '=' in tokens1 and '=' in tokens2:
        try:
            eq_pos1 = tokens1.index('=')
            eq_pos2 = tokens2.index('=')
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–æ = –ø–æ—Ö–æ–∂–∏
            before_eq1 = set(tokens1[:eq_pos1])
            before_eq2 = set(tokens2[:eq_pos2])
            if before_eq1 & before_eq2:
                order_bonus = 0.1
        except ValueError:
            pass
    
    total_score = jaccard + length_bonus * 0.2 + order_bonus
    
    return min(total_score, 1.0)

def is_mathematical_formula_strict_v3(text):
    """
    –£–ñ–ï–°–¢–û–ß–ï–ù–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ñ–æ—Ä–º—É–ª v3
    """
    if not text or len(text.strip()) < 2:
        return False
    
    text = text.strip()
    text = fix_pdf_formula_structure(text)
    
    # –ñ–ï–°–¢–ö–ò–ï –ò–°–ö–õ–Æ–ß–ï–ù–ò–Ø
    strict_exclusions = [
        # –ë–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∑–∞–ø–∏—Å–∏
        r'\b(ISBN|URL|–ì–û–°–¢|–ú–ü–ö|DOI|http|www\.)\b',
        r'\b(–ú–æ—Å–∫–≤–∞|–°–ü–±|–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥|–ü–µ–Ω–∑–∞|–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥)\b',
        r'\d{4}\s*–≥\.',
        r'[–ê-–Ø–Å][–∞-—è—ë]+,\s*[–ê-–Ø–Å]\.\s*[–ê-–Ø–Å]\.',  # –§–∞–º–∏–ª–∏—è, –ò. –û.
        
        # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Ä–∞–∑–¥–µ–ª—ã
        r'^\d+(\.\d+)*\s+[–ê-–Ø–Å]',
        r'^[–ê-–Ø–Å][–ê-–Ø–Å\s]+\d*$',
        r'^\d+\s*$',
        r'^[–ê-–Ø–Å][–ê-–Ø–Å\s]{10,}$',
        
        # –ù–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Ñ—Ä–∞–∑—ã
        r'–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–µ –±–æ–ª–µ–µ',
        r'–¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–µ –º–µ–Ω–µ–µ',
        r'–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å',
        r'—Å–æ–≥–ª–∞—Å–Ω–æ –ì–û–°–¢',
        
        # –°–ø–∏—Å–∫–∏ –∏ –ø–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏—è (–£–°–ò–õ–ï–ù–û)
        r'^\s*[‚Äì‚Äî]\s*[–∞-—è—ë]',  # ‚Äì –ø—Ä–∏–º–µ–Ω—è—Ç—å
        r'^\d+\)\s*[–∞-—è—ë]',
        r'^[–∞-—è—ë]\)\s*',
        
        # –ü–æ—è—Å–Ω–µ–Ω–∏—è –∏ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–∏ (–ù–û–í–û–ï)
        r'^[a-zA-Z–∞-—è—ë]\s*[‚Äì‚Äî]\s*[–∞-—è—ë]',  # c ‚Äì —Å–∫–æ—Ä–æ—Å—Ç—å
        r'[–∞-—è—ë]{5,}\s*[–∞-—è—ë]{5,}',        # –¥–ª–∏–Ω–Ω—ã–µ —Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞
        
        # –°—Ç—Ä–∞–Ω–∏—Ü—ã –∏ —Å—Å—ã–ª–∫–∏
        r'—Å—Ç—Ä\.\s*\d+',
        r'—Å—Ç—Ä–∞–Ω–∏—Ü–∞\s*\d+',
        r'—Ä–∏—Å—É–Ω[–æ–∫|–∫–µ]\s*\d+',
        r'—Ç–∞–±–ª–∏—Ü[–∞|–µ]\s*\d+',
        
        # –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
        r'^.{150,}$',
        
        # –ü—Ä–æ—Å—Ç—ã–µ —á–∏—Å–ª–æ–≤—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        r'^\d+(\s*[",]\s*\d+)*$',  # "1 1", "4 2"
    ]
    
    for pattern in strict_exclusions:
        if re.search(pattern, text, re.IGNORECASE):
            print(f"    ‚ùå –ò–°–ö–õ–Æ–ß–ï–ù–û v3 (–ø–∞—Ç—Ç–µ—Ä–Ω '{pattern[:30]}...'): '{text[:50]}...'")
            return False
    
    # –ü–û–ó–ò–¢–ò–í–ù–´–ï –ü–†–ò–ó–ù–ê–ö–ò - –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ
    math_indicators = [
        r'[=‚â†‚âà‚â§‚â•<>]',                    # –û—Å–Ω–æ–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
        r'[A-Za-z]\s*/\s*[A-Za-z]',     # –î—Ä–æ–±–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        r'[A-Za-z]\^[0-9]',             # –°—Ç–µ–ø–µ–Ω–∏
        r'[A-Za-z][¬≤¬≥‚Å¥‚Åµ‚Å∂‚Å∑‚Å∏‚Åπ]',         # Unicode —Å—Ç–µ–ø–µ–Ω–∏
        r'\b[EIURPVF]\s*=',             # –§–∏–∑–∏—á–µ—Å–∫–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        r'\d+\s*[+\-*/]\s*\d+',        # –ß–∏—Å–ª–æ–≤—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è
        r'(sin|cos|tan|log|ln|exp|sqrt)\s*\(',  # –§—É–Ω–∫—Ü–∏–∏
        r'[Œ±Œ≤Œ≥Œ¥ŒµŒ∂Œ∑Œ∏ŒπŒ∫ŒªŒºŒΩŒæŒøœÄœÅœÉœÑœÖœÜœáœàœâ]',  # –ì—Ä–µ—á–µ—Å–∫–∏–µ –±—É–∫–≤—ã
        r'10\^[+-]?\d+',                # –ù–∞—É—á–Ω–∞—è –Ω–æ—Ç–∞—Ü–∏—è
    ]
    
    indicator_count = sum(1 for pattern in math_indicators if re.search(pattern, text, re.IGNORECASE))
    
    # –¢—Ä–µ–±—É–µ–º –º–∏–Ω–∏–º—É–º 2 –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å—Ç—Ä–æ–∫
    min_indicators = 2 if len(text) < 30 else 1
    
    if indicator_count >= min_indicators:
        print(f"    ‚úÖ –ü–†–ò–ù–Ø–¢–û v3 ({indicator_count} –º–∞—Ç. –ø—Ä–∏–∑–Ω–∞–∫–æ–≤): '{text[:50]}...'")
        return True
    
    print(f"    ‚ùå –û–¢–ö–õ–û–ù–ï–ù–û v3 ({indicator_count} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ < {min_indicators}): '{text[:50]}...'")
    return False

def improved_fuzzy_matching_v3(text1, text2):
    """
    –ö–ê–†–î–ò–ù–ê–õ–¨–ù–û –£–õ–£–ß–®–ï–ù–ù–û–ï —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ v3 —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π
    """
    if not text1 or not text2:
        return 0.0
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
    clean1 = fix_pdf_formula_structure(text1) if any(c in text1 for c in 'ùëàùëÖùêº–í') else fix_docx_formula_structure(text1)
    clean2 = fix_docx_formula_structure(text2)
    
    print(f"      –û—á–∏—â–µ–Ω–Ω—ã–µ: '{clean1}' vs '{clean2}'")
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens1 = tokenize_formula(clean1)
    tokens2 = tokenize_formula(clean2)
    
    print(f"      –¢–æ–∫–µ–Ω—ã: {tokens1} vs {tokens2}")
    
    # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
    if tokens1 == tokens2 and tokens1:
        return 100.0
    
    # –°—Ö–æ–∂–µ—Å—Ç—å –ø–æ —Ç–æ–∫–µ–Ω–∞–º (–æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥)
    token_score = calculate_token_similarity(tokens1, tokens2) * 100
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã –¥–ª—è –ø–æ–¥—Å—Ç—Ä–∞—Ö–æ–≤–∫–∏
    scores = [token_score]
    
    # –û–±—ã—á–Ω—ã–µ fuzzy –º–µ—Ç–æ–¥—ã
    scores.append(fuzz.ratio(clean1, clean2))
    scores.append(fuzz.partial_ratio(clean1, clean2))
    scores.append(fuzz.token_set_ratio(clean1, clean2))
    scores.append(fuzz.token_sort_ratio(clean1, clean2))
    scores.append(fuzz.WRatio(clean1, clean2))
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
    struct_score = variable_overlap_score(clean1, clean2)
    scores.append(struct_score)
    
    best_score = max(scores) / 100.0 if max(scores) > 100 else max(scores) / 100.0
    
    print(f"      Scores: token={token_score:.1f}, ratio={scores[1]}, partial={scores[2]}, token_set={scores[3]}, token_sort={scores[4]}, WRatio={scores[5]}, struct={scores[6]:.1f}")
    print(f"      BEST: {best_score:.2f}")
    
    return best_score

def variable_overlap_score(text1, text2):
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –ø–æ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º –∏ –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞–º
    """
    if not text1 or not text2:
        return 0.0
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (–æ–¥–∏–Ω–æ—á–Ω—ã–µ –±—É–∫–≤—ã)
    vars1 = set(re.findall(r'\b[A-Za-z]\b', text1))
    vars2 = set(re.findall(r'\b[A-Za-z]\b', text2))
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
    ops1 = set(re.findall(r'[=+\-*/^<>‚â§‚â•‚â†‚âà]', text1))
    ops2 = set(re.findall(r'[=+\-*/^<>‚â§‚â•‚â†‚âà]', text2))
    
    # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è
    var_overlap = len(vars1 & vars2) / max(len(vars1 | vars2), 1)
    op_overlap = len(ops1 & ops2) / max(len(ops1 | ops2), 1)
    
    # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞
    return (var_overlap * 0.7 + op_overlap * 0.3) * 100

def normalize_formula_text_advanced(text):
    """
    –ü–†–û–î–í–ò–ù–£–¢–ê–Ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º PDF –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
    """
    if not text:
        return ""
    
    # –°–Ω–∞—á–∞–ª–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
    text = fix_pdf_formula_structure(text) if any(c in text for c in 'ùëàùëÖùêº–í') else fix_docx_formula_structure(text)
    
    # –ë–∞–∑–æ–≤–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    normalized = normalize_formula_text(text)
    
    return normalized

def normalize_formula_text(text):
    """
    –ë–ê–ó–û–í–ê–Ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Ñ–æ—Ä–º—É–ª—ã
    """
    if not text:
        return ""
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    normalized = text.lower()
    
    # –£–±–∏—Ä–∞–µ–º –≤—Å–µ –ø—Ä–æ–±–µ–ª—ã
    normalized = re.sub(r'\s+', '', normalized)
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–µ–º –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã
    symbol_replacements = {
        '√ó': '*', '√∑': '/', '‚àí': '-', '‚Äì': '-', '‚Äî': '-',
        '‚àô': '*', '¬∑': '*', '‚àó': '*', '‚ãÖ': '*',
        '‚âà': '~', '‚â†': '!=', '‚â§': '<=', '‚â•': '>=',
        '‚àû': 'inf', '‚àë': 'sum', '‚à´': 'int', '‚àÇ': 'd',
        '‚àÜ': 'delta', '‚àá': 'nabla', '¬±': '+-', '‚àì': '-+',
        '‚àö': 'sqrt', '‚àõ': 'cbrt', '‚àú': 'qrt',
        '¬≤': '^2', '¬≥': '^3', '‚Å¥': '^4', '‚Åµ': '^5',
        '‚Å∂': '^6', '‚Å∑': '^7', '‚Å∏': '^8', '‚Åπ': '^9',
        '‚Å∞': '^0', '¬π': '^1',
        '¬Ω': '1/2', '‚Öì': '1/3', '¬º': '1/4', '¬æ': '3/4',
        '‚Öõ': '1/8', '‚Öú': '3/8', '‚Öù': '5/8', '‚Öû': '7/8'
    }
    
    for old_symbol, new_symbol in symbol_replacements.items():
        normalized = normalized.replace(old_symbol, new_symbol)
    
    # –£–±–∏—Ä–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã –∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ–ø–µ—Ä–∞—Ç–æ—Ä—ã
    normalized = re.sub(r'[^\w\+\-\*\/\=$$$$\[\]\{\}\^\<\>\!\~]', '', normalized)
    
    # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Å–∏–º–≤–æ–ª—ã
    normalized = re.sub(r'(.)\1+', r'\1', normalized)
    
    return normalized

def extract_omml_formulas_with_lxml(docx_path):
    """
    –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä—è–º–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–æ—Ä–º—É–ª —á–µ—Ä–µ–∑ lxml –∏ –ø–∞—Ä—Å–∏–Ω–≥ OMML
    """
    formulas_info = []
    
    try:
        print("=== –ò–ó–í–õ–ï–ß–ï–ù–ò–ï OMML –§–û–†–ú–£–õ –ß–ï–†–ï–ó LXML ===")
        
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º DOCX –∫–∞–∫ ZIP –∞—Ä—Ö–∏–≤
        with zipfile.ZipFile(docx_path, 'r') as docx_zip:
            # –ß–∏—Ç–∞–µ–º document.xml
            try:
                document_xml = docx_zip.read('word/document.xml')
                print("‚úì –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω word/document.xml")
            except KeyError:
                print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω word/document.xml")
                return formulas_info
            
            # –ü–∞—Ä—Å–∏–º XML —Å –ø–æ–º–æ—â—å—é lxml
            try:
                root = etree.fromstring(document_xml)
                print("‚úì XML —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ XML: {e}")
                return formulas_info
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∏–º–µ–Ω
            namespaces = {
                'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main',
                'm': 'http://schemas.openxmlformats.org/officeDocument/2006/math',
                'mc': 'http://schemas.openxmlformats.org/markup-compatibility/2006'
            }
            
            # –ò—â–µ–º –≤—Å–µ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ–±—ä–µ–∫—Ç—ã
            math_elements = root.xpath('.//m:oMath | .//m:oMathPara', namespaces=namespaces)
            print(f"–ù–∞–π–¥–µ–Ω–æ {len(math_elements)} –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ OMML")
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –æ–±—ä–µ–∫—Ç
            for idx, math_elem in enumerate(math_elements):
                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
                    formula_text = extract_text_from_omml(math_elem, namespaces)
                    
                    if formula_text and formula_text.strip():
                        # –°–¢–†–û–ì–ê–Ø –ü–†–û–í–ï–†–ö–ê: —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ñ–æ—Ä–º—É–ª–∞?
                        if not is_mathematical_formula_strict_v3(formula_text.strip()):
                            print(f"  ‚ùå OMML –æ–±—ä–µ–∫—Ç {idx+1} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Ñ–æ—Ä–º—É–ª–æ–π: '{formula_text.strip()}'")
                            continue
                        
                        # –ù–∞—Ö–æ–¥–∏–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ
                        paragraph_elem = math_elem
                        while paragraph_elem is not None and paragraph_elem.tag != f"{{{namespaces['w']}}}p":
                            paragraph_elem = paragraph_elem.getparent()
                        
                        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
                        all_paragraphs = root.xpath('.//w:p', namespaces=namespaces)
                        paragraph_idx = 0
                        if paragraph_elem is not None:
                            try:
                                paragraph_idx = all_paragraphs.index(paragraph_elem)
                            except ValueError:
                                paragraph_idx = idx  # Fallback
                        
                        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                        context_before, context_after = get_formula_context_from_xml(
                            root, paragraph_elem, namespaces, paragraph_idx
                        )
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
                        is_centered = check_formula_alignment_in_xml(paragraph_elem, namespaces)
                        
                        formulas_info.append({
                            'paragraph_idx': paragraph_idx,
                            'text': formula_text.strip(),
                            'normalized_text': normalize_formula_text_advanced(formula_text),
                            'context_before': context_before,
                            'context_after': context_after,
                            'has_math_object': True,
                            'is_centered': is_centered,
                            'position_in_doc': paragraph_idx,
                            'type': 'omml_extracted',
                            'confidence': 'high',
                            'extraction_method': 'lxml_omml'
                        })
                        
                        print(f"  ‚úÖ OMML —Ñ–æ—Ä–º—É–ª–∞ {idx+1}: '{formula_text.strip()}'")
                        print(f"      –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è: '{normalize_formula_text_advanced(formula_text)}'")
                        print(f"      –ü–∞—Ä–∞–≥—Ä–∞—Ñ: {paragraph_idx}, –¶–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞: {is_centered}")
                    
                except Exception as e:
                    print(f"  ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ OMML —Ñ–æ—Ä–º—É–ª—ã {idx+1}: {e}")
                    continue
    
    except Exception as e:
        print(f"‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è OMML: {e}")
    
    print(f"=== –ò–ó–í–õ–ï–ß–ï–ù–û {len(formulas_info)} –ù–ê–°–¢–û–Ø–©–ò–• OMML –§–û–†–ú–£–õ ===")
    return formulas_info

def extract_text_from_omml(math_elem, namespaces):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ OMML –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞
    """
    try:
        # –ò—â–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–º –æ–±—ä–µ–∫—Ç–µ
        text_elements = math_elem.xpath('.//m:t', namespaces=namespaces)
        
        formula_parts = []
        for text_elem in text_elements:
            if text_elem.text:
                formula_parts.append(text_elem.text)
        
        # –¢–∞–∫–∂–µ –∏—â–µ–º –æ–±—ã—á–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –≤–Ω—É—Ç—Ä–∏ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∏
        w_text_elements = math_elem.xpath('.//w:t', namespaces=namespaces)
        for text_elem in w_text_elements:
            if text_elem.text:
                formula_parts.append(text_elem.text)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —á–∞—Å—Ç–∏ —Ñ–æ—Ä–º—É–ª—ã
        formula_text = ''.join(formula_parts)
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        # –ò—â–µ–º –¥—Ä–æ–±–∏
        fractions = math_elem.xpath('.//m:f', namespaces=namespaces)
        for frac in fractions:
            num_text = ''.join([t.text or '' for t in frac.xpath('.//m:num//m:t', namespaces=namespaces)])
            den_text = ''.join([t.text or '' for t in frac.xpath('.//m:den//m:t', namespaces=namespaces)])
            if num_text and den_text:
                formula_text += f"({num_text})/({den_text})"
        
        # –ò—â–µ–º —Å—Ç–µ–ø–µ–Ω–∏
        superscripts = math_elem.xpath('.//m:sSup', namespaces=namespaces)
        for sup in superscripts:
            base_text = ''.join([t.text or '' for t in sup.xpath('.//m:e//m:t', namespaces=namespaces)])
            sup_text = ''.join([t.text or '' for t in sup.xpath('.//m:sup//m:t', namespaces=namespaces)])
            if base_text and sup_text:
                formula_text += f"{base_text}^{sup_text}"
        
        return formula_text
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ OMML: {e}")
        return ""

def get_formula_context_from_xml(root, paragraph_elem, namespaces, paragraph_idx):
    """
    –ü–æ–ª—É—á–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ñ–æ—Ä–º—É–ª—ã –∏–∑ XML
    """
    try:
        all_paragraphs = root.xpath('.//w:p', namespaces=namespaces)
        
        context_before = ""
        context_after = ""
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ —Ñ–æ—Ä–º—É–ª—ã
        for i in range(max(0, paragraph_idx - 2), paragraph_idx):
            if i < len(all_paragraphs):
                para_text = get_paragraph_text_from_xml(all_paragraphs[i], namespaces)
                context_before += para_text + " "
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —Ñ–æ—Ä–º—É–ª—ã
        for i in range(paragraph_idx + 1, min(len(all_paragraphs), paragraph_idx + 3)):
            if i < len(all_paragraphs):
                para_text = get_paragraph_text_from_xml(all_paragraphs[i], namespaces)
                context_after += para_text + " "
        
        return context_before.strip()[-100:], context_after.strip()[:100]
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
        return "", ""

def get_paragraph_text_from_xml(paragraph_elem, namespaces):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞ XML
    """
    try:
        text_elements = paragraph_elem.xpath('.//w:t', namespaces=namespaces)
        return ''.join([elem.text or '' for elem in text_elements])
    except:
        return ""

def check_formula_alignment_in_xml(paragraph_elem, namespaces):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Ñ–æ—Ä–º—É–ª—ã –≤ XML
    """
    try:
        if paragraph_elem is None:
            return False
        
        # –ò—â–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
        jc_elements = paragraph_elem.xpath('.//w:jc', namespaces=namespaces)
        for jc in jc_elements:
            val = jc.get(f"{{{namespaces['w']}}}val")
            if val == "center":
                return True
        
        return False
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è: {e}")
        return False

def find_docx_formulas_with_positions(docx_path):
    """
    –§–ò–ù–ê–õ–¨–ù–ê–Ø –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ —Ñ–æ—Ä–º—É–ª –≤ DOCX v3
    """
    print("=== –ü–û–ò–°–ö –§–û–†–ú–£–õ –í DOCX (–§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø v3) ===")
    
    # –ú–µ—Ç–æ–¥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ lxml –∏ OMML (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π)
    omml_formulas = extract_omml_formulas_with_lxml(docx_path)
    
    # –ú–µ—Ç–æ–¥ 2: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–µ—Ä–µ–∑ python-docx —Å –£–ñ–ï–°–¢–û–ß–ï–ù–ù–û–ô —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π v3
    try:
        doc = Document(docx_path)
        docx_formulas = []
        
        print("\n=== –†–ï–ó–ï–†–í–ù–û–ï –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –ß–ï–†–ï–ó PYTHON-DOCX (v3) ===")
        
        for para_idx, paragraph in enumerate(doc.paragraphs):
            para_text = paragraph.text.strip()
            
            if not para_text:
                continue
            
            print(f"  –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ {para_idx}: '{para_text[:50]}{'...' if len(para_text) > 50 else ''}'")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —É–∂–µ —ç—Ç–∞ —Ñ–æ—Ä–º—É–ª–∞ –≤ OMML —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
            found_in_omml = False
            for omml_formula in omml_formulas:
                if (omml_formula['paragraph_idx'] == para_idx or 
                    abs(omml_formula['paragraph_idx'] - para_idx) <= 1):
                    found_in_omml = True
                    print(f"    ‚ö†Ô∏è –£–∂–µ –Ω–∞–π–¥–µ–Ω–æ –≤ OMML")
                    break
            
            if not found_in_omml and is_mathematical_formula_strict_v3(para_text):
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                context_before = ""
                context_after = ""
                
                for i in range(max(0, para_idx - 2), para_idx):
                    context_before += doc.paragraphs[i].text + " "
                
                for i in range(para_idx + 1, min(len(doc.paragraphs), para_idx + 3)):
                    context_after += doc.paragraphs[i].text + " "
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç—å
                is_centered = False
                if paragraph.paragraph_format.alignment is not None:
                    from docx.enum.text import WD_ALIGN_PARAGRAPH
                    is_centered = paragraph.paragraph_format.alignment == WD_ALIGN_PARAGRAPH.CENTER
                
                docx_formulas.append({
                    'paragraph_idx': para_idx,
                    'text': para_text,
                    'normalized_text': normalize_formula_text_advanced(para_text),
                    'context_before': context_before.strip()[-100:],
                    'context_after': context_after.strip()[:100],
                    'has_math_object': False,
                    'is_centered': is_centered,
                    'position_in_doc': para_idx,
                    'type': 'text_formula',
                    'confidence': 'medium',
                    'extraction_method': 'python_docx'
                })
                
                print(f"    ‚úÖ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–ê–Ø –§–û–†–ú–£–õ–ê: –ø–∞—Ä–∞–≥—Ä–∞—Ñ {para_idx}")
                print(f"        –¢–µ–∫—Å—Ç: '{para_text}'")
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (OMML –∏–º–µ–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        all_formulas = omml_formulas + docx_formulas
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
        all_formulas = omml_formulas
    
    print(f"\n=== –ò–¢–û–ì–û –ù–ê–ô–î–ï–ù–û {len(all_formulas)} –ù–ê–°–¢–û–Ø–©–ò–• –§–û–†–ú–£–õ –í DOCX ===")
    print(f"OMML —Ñ–æ—Ä–º—É–ª: {len(omml_formulas)}")
    print(f"–¢–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–æ—Ä–º—É–ª: {len(all_formulas) - len(omml_formulas)}")
    
    # –í—ã–≤–æ–¥–∏–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ñ–æ—Ä–º—É–ª—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    for i, formula in enumerate(all_formulas, 1):
        print(f"  üìê –§–æ—Ä–º—É–ª–∞ {i}: '{formula['text']}' (–º–µ—Ç–æ–¥: {formula['extraction_method']}, –ø–æ–∑: {formula['position_in_doc']})")
    
    return all_formulas

def match_pdf_formulas_to_docx(pdf_formulas, docx_formulas):
    """
    –§–ò–ù–ê–õ–¨–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º—É–ª v3
    """
    matching = {}
    
    if not pdf_formulas or not docx_formulas:
        print(f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è: PDF={len(pdf_formulas or [])}, DOCX={len(docx_formulas or [])}")
        return matching
    
    print(f"=== –§–ò–ù–ê–õ–¨–ù–û–ï –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–ï –§–û–†–ú–£–õ v3 ===")
    print(f"PDF —Ñ–æ—Ä–º—É–ª: {len(pdf_formulas)}")
    print(f"DOCX —Ñ–æ—Ä–º—É–ª: {len(docx_formulas)}")
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º PDF —Ñ–æ—Ä–º—É–ª—ã - —É–±–∏—Ä–∞–µ–º —è–≤–Ω–æ –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ
    filtered_pdf_formulas = []
    for i, pdf_formula in enumerate(pdf_formulas):
        pdf_text = pdf_formula.get('text', '').strip()
        if is_mathematical_formula_strict_v3(pdf_text):
            filtered_pdf_formulas.append((i, pdf_formula))
        else:
            print(f"‚ùå PDF —Ñ–æ—Ä–º—É–ª–∞ {i+1} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∞: '{pdf_text}'")
    
    print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ PDF: {len(filtered_pdf_formulas)} –∏–∑ {len(pdf_formulas)}")
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤—Å–µ —Ñ–æ—Ä–º—É–ª—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    normalized_pdf = []
    normalized_docx = []
    
    for original_idx, pdf_formula in filtered_pdf_formulas:
        cleaned_text = fix_pdf_formula_structure(pdf_formula.get('text', ''))
        normalized_text = normalize_formula_text_advanced(cleaned_text)
        normalized_pdf.append({
            'original_index': original_idx,
            'index': len(normalized_pdf),
            'original_text': pdf_formula.get('text', ''),
            'cleaned_text': cleaned_text,
            'normalized_text': normalized_text,
            'page': pdf_formula.get('page', 1),
            'context': (pdf_formula.get('context_before', '') + ' ' + 
                       pdf_formula.get('context_after', '')).strip()
        })
        print(f"PDF {original_idx+1}: '{pdf_formula.get('text', '')}' -> '{cleaned_text}' -> '{normalized_text}'")
    
    for i, docx_formula in enumerate(docx_formulas):
        cleaned_text = fix_docx_formula_structure(docx_formula.get('text', ''))
        normalized_text = normalize_formula_text_advanced(cleaned_text)
        normalized_docx.append({
            'index': i,
            'original_text': docx_formula.get('text', ''),
            'cleaned_text': cleaned_text,
            'normalized_text': normalized_text,
            'position': docx_formula.get('position_in_doc', i),
            'context': (docx_formula.get('context_before', '') + ' ' + 
                       docx_formula.get('context_after', '')).strip()
        })
        print(f"DOCX {i+1}: '{docx_formula.get('text', '')}' -> '{cleaned_text}' -> '{normalized_text}'")
    
    used_docx = set()
    match_log = []
    
    # –ú–ï–¢–û–î 1: –¢–æ—á–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º—É —Ç–µ–∫—Å—Ç—É
    print("\nüéØ –ú–ï–¢–û–î 1: –¢–æ—á–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ")
    for pdf_item in normalized_pdf:
        if pdf_item['original_index'] in matching:
            continue
            
        for docx_item in normalized_docx:
            if docx_item['index'] in used_docx:
                continue
                
            if (pdf_item['normalized_text'] == docx_item['normalized_text'] and 
                pdf_item['normalized_text'] and 
                len(pdf_item['normalized_text']) > 2):
                
                matching[pdf_item['original_index']] = docx_item['index']
                used_docx.add(docx_item['index'])
                
                log_entry = {
                    'method': 'exact_match',
                    'pdf_idx': pdf_item['original_index'],
                    'docx_idx': docx_item['index'],
                    'pdf_text': pdf_item['original_text'],
                    'docx_text': docx_item['original_text'],
                    'score': 1.0,
                    'reason': '–¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞'
                }
                match_log.append(log_entry)
                
                print(f"  ‚úÖ –¢–û–ß–ù–û–ï –°–û–í–ü–ê–î–ï–ù–ò–ï: PDF {pdf_item['original_index']+1} -> DOCX {docx_item['index']+1}")
                print(f"      PDF: '{pdf_item['original_text']}'")
                print(f"      DOCX: '{docx_item['original_text']}'")
                break
    
    # –ú–ï–¢–û–î 2: –£–ª—É—á—à–µ–Ω–Ω–æ–µ Fuzzy matching v3 —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–µ–π
    print("\nüîç –ú–ï–¢–û–î 2: –£–ª—É—á—à–µ–Ω–Ω–æ–µ Fuzzy matching v3")
    for pdf_item in normalized_pdf:
        if pdf_item['original_index'] in matching:
            continue
            
        best_match = None
        best_score = 0
        
        for docx_item in normalized_docx:
            if docx_item['index'] in used_docx:
                continue
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ v3
            content_score = improved_fuzzy_matching_v3(pdf_item['original_text'], docx_item['original_text'])
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ —Å—Ö–æ–∂–µ—Å—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context_score = 0
            if pdf_item['context'] and docx_item['context']:
                context_score = fuzz.partial_ratio(pdf_item['context'], docx_item['context']) / 100.0 * 0.1
            
            total_score = content_score + context_score
            
            print(f"    PDF {pdf_item['original_index']+1} vs DOCX {docx_item['index']+1}: —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ={content_score:.2f}, –∫–æ–Ω—Ç–µ–∫—Å—Ç={context_score:.2f}, –∏—Ç–æ–≥–æ={total_score:.2f}")
            
            if total_score > best_score and total_score > 0.70:  # –ü–û–ù–ò–ñ–ï–ù –ø–æ—Ä–æ–≥ –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏
                best_score = total_score
                best_match = docx_item
        
        if best_match:
            matching[pdf_item['original_index']] = best_match['index']
            used_docx.add(best_match['index'])
            
            log_entry = {
                'method': 'fuzzy_match',
                'pdf_idx': pdf_item['original_index'],
                'docx_idx': best_match['index'],
                'pdf_text': pdf_item['original_text'],
                'docx_text': best_match['original_text'],
                'score': best_score,
                'reason': f'Fuzzy matching v3 —Å –±–∞–ª–ª–æ–º {best_score:.2f}'
            }
            match_log.append(log_entry)
            
            print(f"  ‚úÖ FUZZY –°–û–í–ü–ê–î–ï–ù–ò–ï v3: PDF {pdf_item['original_index']+1} -> DOCX {best_match['index']+1} (–±–∞–ª–ª: {best_score:.2f})")
            print(f"      PDF: '{pdf_item['original_text']}'")
            print(f"      DOCX: '{best_match['original_text']}'")
    
    # –ú–ï–¢–û–î 3: –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Ö–æ—Ä–æ—à–µ–π —Å—Ö–æ–∂–µ—Å—Ç–∏)
    print("\nüìç –ú–ï–¢–û–î 3: –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ")
    remaining_pdf = [item for item in normalized_pdf if item['original_index'] not in matching]
    remaining_docx = [item for item in normalized_docx if item['index'] not in used_docx]
    
    print(f"–û—Å—Ç–∞–ª–æ—Å—å –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö: PDF={len(remaining_pdf)}, DOCX={len(remaining_docx)}")
    
    if len(remaining_pdf) > 0 and len(remaining_docx) > 0 and abs(len(remaining_pdf) - len(remaining_docx)) <= 2:
        remaining_pdf.sort(key=lambda x: x['page'])
        remaining_docx.sort(key=lambda x: x['position'])
        
        min_count = min(len(remaining_pdf), len(remaining_docx))
        
        for i in range(min_count):
            pdf_item = remaining_pdf[i]
            docx_item = remaining_docx[i]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é —Å—Ö–æ–∂–µ—Å—Ç—å
            similarity = improved_fuzzy_matching_v3(pdf_item['original_text'], docx_item['original_text'])
            
            if similarity > 0.3:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å—Ö–æ–∂–µ—Å—Ç—å
                matching[pdf_item['original_index']] = docx_item['index']
                used_docx.add(docx_item['index'])
                
                log_entry = {
                    'method': 'position_match',
                    'pdf_idx': pdf_item['original_index'],
                    'docx_idx': docx_item['index'],
                    'pdf_text': pdf_item['original_text'],
                    'docx_text': docx_item['original_text'],
                    'score': 0.5 + similarity * 0.3,
                    'reason': f'–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å —Å—Ö–æ–∂–µ—Å—Ç—å—é {similarity:.2f}'
                }
                match_log.append(log_entry)
                
                print(f"  ‚úÖ –ü–û–ó–ò–¶–ò–û–ù–ù–û–ï –°–û–í–ü–ê–î–ï–ù–ò–ï: PDF {pdf_item['original_index']+1} -> DOCX {docx_item['index']+1} (—Å—Ö–æ–∂–µ—Å—Ç—å: {similarity:.2f})")
                print(f"      PDF: '{pdf_item['original_text']}' (—Å—Ç—Ä. {pdf_item['page']})")
                print(f"      DOCX: '{docx_item['original_text']}' (–ø–æ–∑. {docx_item['position']})")
            else:
                print(f"  ‚ùå –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–æ –∏–∑-–∑–∞ –Ω–∏–∑–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ ({similarity:.2f})")
    else:
        print("  ‚ö†Ô∏è –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ")
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ–æ—Ä–º—É–ª
    print("\n‚ùå –ù–ï–°–û–ü–û–°–¢–ê–í–õ–ï–ù–ù–´–ï –§–û–†–ú–£–õ–´:")
    for pdf_item in normalized_pdf:
        if pdf_item['original_index'] not in matching:
            print(f"  PDF {pdf_item['original_index']+1}: '{pdf_item['original_text']}' (—Å—Ç—Ä. {pdf_item['page']})")
    
    for docx_item in normalized_docx:
        if docx_item['index'] not in used_docx:
            print(f"  DOCX {docx_item['index']+1}: '{docx_item['original_text']}' (–ø–æ–∑. {docx_item['position']})")
    
    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n=== –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê v3 ===")
    print(f"–í—Å–µ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π: {len(matching)}")
    print(f"–¢–æ—á–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len([log for log in match_log if log['method'] == 'exact_match'])}")
    print(f"Fuzzy —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len([log for log in match_log if log['method'] == 'fuzzy_match'])}")
    print(f"–ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {len([log for log in match_log if log['method'] == 'position_match'])}")
    
    return matching

# –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
# (–ö–æ–ø–∏—Ä—É–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –≤–µ—Ä—Å–∏–∏)

def remove_existing_comments(docx_path, output_path):
    """–ë–ï–ó–û–ü–ê–°–ù–û–ï —É–¥–∞–ª–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–∏–º–µ—á–∞–Ω–∏–π –∏–∑ DOCX —Ñ–∞–π–ª–∞"""
    try:
        print(f"–£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–º–µ—á–∞–Ω–∏–π –∏–∑ {docx_path}")
        
        try:
            doc = Document(docx_path)
            
            for paragraph in doc.paragraphs:
                for run in paragraph.runs:
                    run_element = run._element
                    
                    comment_elements = []
                    for child in run_element:
                        if child.tag.endswith('}commentRangeStart') or \
                           child.tag.endswith('}commentRangeEnd') or \
                           child.tag.endswith('}commentReference'):
                            comment_elements.append(child)
                    
                    for elem in comment_elements:
                        run_element.remove(elem)
            
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        for paragraph in cell.paragraphs:
                            for run in paragraph.runs:
                                run_element = run._element
                                comment_elements = []
                                for child in run_element:
                                    if child.tag.endswith('}commentRangeStart') or \
                                       child.tag.endswith('}commentRangeEnd') or \
                                       child.tag.endswith('}commentReference'):
                                        comment_elements.append(child)
                                
                                for elem in comment_elements:
                                    run_element.remove(elem)
            
            doc.save(output_path)
            print("‚úì –ú–µ—Ç–æ–¥ 1 (–±–µ–∑–æ–ø–∞—Å–Ω—ã–π python-docx) –≤—ã–ø–æ–ª–Ω–µ–Ω")
            
            try:
                test_doc = Document(output_path)
                print("‚úì –§–∞–π–ª –ø—Ä–æ—à–µ–ª –ø—Ä–æ–≤–µ—Ä–∫—É —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏")
                return True
            except Exception as e:
                print(f"‚ö†Ô∏è –§–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥–µ–Ω –ø–æ—Å–ª–µ –º–µ—Ç–æ–¥–∞ 1: {e}")
                
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –º–µ—Ç–æ–¥–∞ 1: {e}")
        
        print("–ü—Ä–∏–º–µ–Ω—è–µ–º –º–µ—Ç–æ–¥ 2 (–æ—Å—Ç–æ—Ä–æ–∂–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å ZIP)")
        
        temp_path = output_path + '.temp'
        import shutil
        shutil.copy2(docx_path, temp_path)
        
        with zipfile.ZipFile(temp_path, 'r') as zip_read:
            with zipfile.ZipFile(output_path, 'w', zipfile.ZIP_DEFLATED) as zip_write:
                for item in zip_read.infolist():
                    data = zip_read.read(item.filename)
                    
                    if item.filename == 'word/comments.xml':
                        print("  –£–¥–∞–ª–µ–Ω word/comments.xml")
                        continue
                    
                    if item.filename == 'word/document.xml':
                        try:
                            content = data.decode('utf-8')
                            
                            content = re.sub(r'<w:commentRangeStart[^>]*?/>', '', content)
                            content = re.sub(r'<w:commentRangeEnd[^>]*?/>', '', content)
                            content = re.sub(r'<w:commentReference[^>]*?/>', '', content)
                            
                            data = content.encode('utf-8')
                            print("  –û–±—Ä–∞–±–æ—Ç–∞–Ω word/document.xml (—Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è)")
                            
                        except Exception as e:
                            print(f"  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ document.xml: {e}")
                    
                    elif item.filename == '[Content_Types].xml':
                        try:
                            content = data.decode('utf-8')
                            lines = content.split('\n')
                            filtered_lines = [line for line in lines if 'comments' not in line.lower()]
                            data = '\n'.join(filtered_lines).encode('utf-8')
                            print("  –û–±—Ä–∞–±–æ—Ç–∞–Ω [Content_Types].xml")
                        except Exception as e:
                            print(f"  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ [Content_Types].xml: {e}")
                    
                    elif item.filename == 'word/_rels/document.xml.rels':
                        try:
                            content = data.decode('utf-8')
                            lines = content.split('\n')
                            filtered_lines = [line for line in lines if 'comments' not in line.lower()]
                            data = '\n'.join(filtered_lines).encode('utf-8')
                            print("  –û–±—Ä–∞–±–æ—Ç–∞–Ω word/_rels/document.xml.rels")
                        except Exception as e:
                            print(f"  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ document.xml.rels: {e}")
                    
                    zip_write.writestr(item, data)
        
        os.unlink(temp_path)
        
        try:
            test_doc = Document(output_path)
            print("‚úì –ú–µ—Ç–æ–¥ 2 (ZIP) –≤—ã–ø–æ–ª–Ω–µ–Ω –∏ —Ñ–∞–π–ª –ø—Ä–æ—à–µ–ª –ø—Ä–æ–≤–µ—Ä–∫—É")
            return True
        except Exception as e:
            print(f"‚ö†Ô∏è –§–∞–π–ª –ø–æ–≤—Ä–µ–∂–¥–µ–Ω –ø–æ—Å–ª–µ –º–µ—Ç–æ–¥–∞ 2: {e}")
            
    except Exception as e:
        print(f"–ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –ø—Ä–∏–º–µ—á–∞–Ω–∏–π: {e}")
        try:
            import shutil
            shutil.copy2(docx_path, output_path)
            return False
        except:
            return False

def find_docx_images_with_positions(docx_path):
    """–ù–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ DOCX —Å –∏—Ö –ø–æ–∑–∏—Ü–∏—è–º–∏ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
    try:
        doc = Document(docx_path)
        images_info = []
        
        for para_idx, paragraph in enumerate(doc.paragraphs):
            for run_idx, run in enumerate(paragraph.runs):
                if run._element.xpath('.//pic:pic'):
                    context_before = ""
                    context_after = ""
                    
                    for i in range(max(0, para_idx - 2), para_idx):
                        context_before += doc.paragraphs[i].text + " "
                    
                    for i in range(run_idx):
                        context_before += paragraph.runs[i].text + " "
                    
                    for i in range(run_idx + 1, len(paragraph.runs)):
                        context_after += paragraph.runs[i].text + " "
                    
                    for i in range(para_idx + 1, min(len(doc.paragraphs), para_idx + 3)):
                        context_after += doc.paragraphs[i].text + " "
                    
                    images_info.append({
                        'paragraph_idx': para_idx,
                        'run_idx': run_idx,
                        'context_before': context_before.strip()[-100:],
                        'context_after': context_after.strip()[:100],
                        'paragraph_text': paragraph.text,
                        'position_in_doc': para_idx
                    })
        
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(images_info)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ DOCX")
        for i, img in enumerate(images_info):
            print(f"  –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1}: –ø–∞—Ä–∞–≥—Ä–∞—Ñ {img['paragraph_idx']}, –∫–æ–Ω—Ç–µ–∫—Å—Ç: '{img['context_before'][-50:]}' ... '{img['context_after'][:50]}'")
        
        return images_info
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ DOCX: {e}")
        return []

def match_pdf_images_to_docx(pdf_images, docx_images):
    """–£–õ–£–ß–®–ï–ù–ù–û–ï —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π PDF —Å DOCX"""
    matching = {}
    
    if not pdf_images or not docx_images:
        return matching
    
    print(f"–°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ: {len(pdf_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π PDF —Å {len(docx_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ DOCX")
    
    pdf_by_page = {}
    for i, pdf_img in enumerate(pdf_images):
        page = pdf_img.get('page', 1)
        if page not in pdf_by_page:
            pdf_by_page[page] = []
        pdf_by_page[page].append((i, pdf_img))
    
    docx_idx = 0
    for page in sorted(pdf_by_page.keys()):
        page_images = pdf_by_page[page]
        
        for pdf_idx, pdf_img in page_images:
            if docx_idx < len(docx_images):
                matching[pdf_idx] = docx_idx
                print(f"  PDF –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {pdf_idx} (—Å—Ç—Ä. {page}) -> DOCX –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {docx_idx}")
                docx_idx += 1
            else:
                print(f"  PDF –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {pdf_idx} (—Å—Ç—Ä. {page}) -> –ù–ï–¢ –°–û–û–¢–í–ï–¢–°–¢–í–ò–Ø –≤ DOCX")
    
    return matching

def calculate_formula_content_similarity(text1, text2):
    """–£–õ–£–ß–®–ï–ù–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Ñ–æ—Ä–º—É–ª"""
    if not text1 or not text2:
        return 0.0
    
    norm1 = normalize_formula_text_advanced(text1)
    norm2 = normalize_formula_text_advanced(text2)
    
    if norm1 == norm2:
        return 1.0
    
    clean1 = ''.join(norm1.split())
    clean2 = ''.join(norm2.split())
    
    if clean1 == clean2:
        return 0.95
    
    if clean1 and clean2:
        if clean1 in clean2 or clean2 in clean1:
            shorter = min(len(clean1), len(clean2))
            longer = max(len(clean1), len(clean2))
            if shorter >= 3:
                return (shorter / longer) * 0.8
    
    set1 = set(clean1)
    set2 = set(clean2)
    
    if not set1 or not set2:
        return 0.0
    
    common_chars = set1 & set2
    all_chars = set1 | set2
    
    base_similarity = len(common_chars) / len(all_chars)
    
    math_ops = set('=+-*/^()[]{}')
    math1 = set1 & math_ops
    math2 = set2 & math_ops
    
    if math1 and math2:
        common_math = math1 & math2
        math_bonus = len(common_math) * 0.2
        base_similarity += math_bonus
    
    return min(base_similarity, 1.0)

def calculate_context_similarity(context1, context2):
    """–£–õ–£–ß–®–ï–ù–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    if not context1 or not context2:
        return 0.0
    
    words1 = set(re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', context1.lower()))
    words2 = set(re.findall(r'\b[–∞-—è—ëa-z]{3,}\b', context2.lower()))
    
    if not words1 or not words2:
        return 0.0
    
    common_words = words1 & words2
    all_words = words1 | words2
    
    return len(common_words) / len(all_words) if all_words else 0.0

def try_commenting(doc, para_index, comment_text, method="first"):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –ø—Ä–∏–º–µ—á–∞–Ω–∏–µ –∫ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—É —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏"""
    try:
        if para_index >= len(doc.paragraphs):
            return False
        
        para = doc.paragraphs[para_index]
        runs = para.runs
        
        if not runs:
            runs = [para.add_run(" ")]
        
        if method == "first":
            target_runs = runs[0]
        elif method == "last":
            target_runs = runs[-1]
        else:
            target_runs = runs
        
        comment = doc.add_comment(runs=target_runs, text=comment_text, author="–ì–û–°–¢ –ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä", initials="–ì–ê")
        print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ –ø—Ä–∏–º–µ—á–∞–Ω–∏–µ –º–µ—Ç–æ–¥–æ–º {method}: {comment_text[:50]}...")
        return True
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏–º–µ—á–∞–Ω–∏—è –º–µ—Ç–æ–¥–æ–º {method}: {e}")
        return False

def annotate_docx_with_issues(docx_path, pdf_path, analysis_results, output_path):
    """–§–ò–ù–ê–õ–¨–ù–ê–Ø –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è DOCX —Ñ–∞–π–ª–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ñ–æ—Ä–º—É–ª v3"""
    try:
        doc = Document(docx_path)
        
        docx_images = find_docx_images_with_positions(docx_path)
        docx_formulas = find_docx_formulas_with_positions(docx_path)
        
        from matcher.docx_checks import check_docx_hyphenation, check_docx_double_spaces, check_docx_margins
        
        docx_issues = []
        docx_issues.extend(check_docx_hyphenation(docx_path))
        docx_issues.extend(check_docx_double_spaces(docx_path))
        docx_issues.extend(check_docx_margins(docx_path))
        
        analysis_results['docx_checks'] = docx_issues
        
        if analysis_results.get("images") and docx_images:
            pdf_images = analysis_results["images"]
            image_matching = match_pdf_images_to_docx(pdf_images, docx_images)
            
            print(f"=== –î–û–ë–ê–í–õ–ï–ù–ò–ï –ü–†–ò–ú–ï–ß–ê–ù–ò–ô –ö –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø–ú ===")
            
            for pdf_idx, pdf_img in enumerate(pdf_images):
                if not pdf_img.get('gost_compliant', True):
                    issues = []
                    if not pdf_img.get('is_centered', True):
                        issues.append("–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ")
                    if not pdf_img.get('margins_ok', True):
                        issues.append("–Ω–∞—Ä—É—à–µ–Ω—ã –ø–æ–ª—è")
                    if not pdf_img.get('has_empty_line', True):
                        issues.append("–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–µ—Ä–µ–¥ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º")
                    
                    if issues:
                        comment_text = f"üì∑ –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï (—Å—Ç—Ä. {pdf_img['page']}): {'; '.join(issues)}"
                        
                        if pdf_idx in image_matching:
                            docx_idx = image_matching[pdf_idx]
                            if docx_idx < len(docx_images):
                                docx_img = docx_images[docx_idx]
                                para_idx = docx_img['paragraph_idx']
                                
                                print(f"–î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—á–∞–Ω–∏–µ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é {docx_idx+1} –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ {para_idx}")
                                try_commenting(doc, para_idx, comment_text, "first")
                            else:
                                print(f"–û–®–ò–ë–ö–ê: docx_idx {docx_idx} –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞")
                        else:
                            print(f"PDF –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {pdf_idx} –Ω–µ –∏–º–µ–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ DOCX")
        
        if analysis_results.get("formulas") and docx_formulas:
            pdf_formulas = analysis_results["formulas"]
            formula_matching = match_pdf_formulas_to_docx(pdf_formulas, docx_formulas)
            
            print(f"=== –î–û–ë–ê–í–õ–ï–ù–ò–ï –ü–†–ò–ú–ï–ß–ê–ù–ò–ô –ö –§–û–†–ú–£–õ–ê–ú ===")
            
            for pdf_idx, pdf_formula in enumerate(pdf_formulas):
                if not pdf_formula.get('gost_compliant', True):
                    issues = []
                    if not pdf_formula.get('is_centered', True):
                        issues.append("—Ñ–æ—Ä–º—É–ª–∞ –Ω–µ —Ü–µ–Ω—Ç—Ä–∏—Ä–æ–≤–∞–Ω–∞")
                    if not pdf_formula.get('margins_ok', True):
                        issues.append("–Ω–∞—Ä—É—à–µ–Ω—ã –ø–æ–ª—è")
                    if not pdf_formula.get('has_numbering', True):
                        issues.append("–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –Ω—É–º–µ—Ä–∞—Ü–∏—è")
                    
                    if issues:
                        comment_text = f"üßÆ –§–û–†–ú–£–õ–ê (—Å—Ç—Ä. {pdf_formula['page']}): {'; '.join(issues)} | –¢–µ–∫—Å—Ç: '{pdf_formula.get('text', '')[:30]}...'"
                        
                        if pdf_idx in formula_matching:
                            docx_idx = formula_matching[pdf_idx]
                            if docx_idx < len(docx_formulas):
                                docx_formula = docx_formulas[docx_idx]
                                para_idx = docx_formula['paragraph_idx']
                                
                                print(f"–î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—á–∞–Ω–∏–µ –∫ —Ñ–æ—Ä–º—É–ª–µ {docx_idx+1} –≤ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–µ {para_idx}")
                                try_commenting(doc, para_idx, comment_text, "first")
                            else:
                                print(f"–û–®–ò–ë–ö–ê: docx_idx {docx_idx} –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞")
                        else:
                            print(f"PDF —Ñ–æ—Ä–º—É–ª–∞ {pdf_idx} –Ω–µ –∏–º–µ–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ DOCX")
        
        # –û—Å—Ç–∞–ª—å–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è —Ç–∞–±–ª–∏—Ü, —Ç–µ–∫—Å—Ç–∞ –∏ —Ç.–¥. –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
        
        doc.save(output_path)
        return True
        
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ DOCX: {e}")
        return False
